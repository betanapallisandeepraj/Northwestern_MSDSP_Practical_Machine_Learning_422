{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid method: Recursive feature elimination\n",
    "\n",
    "This method consists of the following steps:\n",
    "\n",
    "1) Rank the features according to their importance derived from a machine learning algorithm: it can be tree importance, or LASSO / Ridge, or the linear / logistic regression coefficients.\n",
    "\n",
    "2) Remove one feature -the least important- and build a machine learning algorithm utilising the remaining features.\n",
    "\n",
    "3) Calculate a performance metric of your choice: roc-auc, mse, rmse, accuracy.\n",
    "\n",
    "4) If the metric decreases by more of an arbitrarily set threshold, then that feature is important and should be kept. Otherwise, we can remove that feature.\n",
    "\n",
    "5) Repeat steps 2-4 until all features have been removed (and therefore evaluated) and the drop in performance assessed.\n",
    "\n",
    "\n",
    "I call this a hybrid method because:\n",
    "\n",
    "- it combines the importance derived from the machine learning algorithm like embedded methods,\n",
    "- and it removes as well one feature at a time, and calculates a new metric based on the new subset of features and the machine learning algorithm of choice, like wrapper methods.\n",
    "\n",
    "The difference between this method and the step backwards feature selection we learned in previous lectures lies in that it does not remove all features first in order to determine which one to remove. It removes the least important one, based on the machine learning model derived important. And then, it makes an assessment as to whether that feature should be removed or not. So it removes each feature only once during selection, whereas step backward feature selection removes all the features at each step of selection.\n",
    "\n",
    "This method is therefore faster than wrapper methods and generally better than embedded methods. In practice it works extremely well. It does also account for correlations (depending on how stringent you set the arbitrary performance drop threshold). On the downside, the drop in performance assessed to decide whether the feature should be kept or removed, is set arbitrarily. The smaller the drop the more features will be selected, and vice versa.\n",
    "\n",
    "\n",
    "**Note** For the demonstration, lets use XGBoost, but this method is useful for any machine learning algorithm. In fact, the importance of the features are determined specifically for the algorithm used. Therefore, different algorithms may return different subsets of important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 133)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "data = pd.read_csv('D:/Machine Learning_Class/Feature Selection/Data/bnp-paribas-cardif-claims-management/paribas.csv', nrows=50000)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>target</th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>...</th>\n",
       "      <th>v122</th>\n",
       "      <th>v123</th>\n",
       "      <th>v124</th>\n",
       "      <th>v125</th>\n",
       "      <th>v126</th>\n",
       "      <th>v127</th>\n",
       "      <th>v128</th>\n",
       "      <th>v129</th>\n",
       "      <th>v130</th>\n",
       "      <th>v131</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.335739</td>\n",
       "      <td>8.727474</td>\n",
       "      <td>C</td>\n",
       "      <td>3.921026</td>\n",
       "      <td>7.915266</td>\n",
       "      <td>2.599278</td>\n",
       "      <td>3.176895</td>\n",
       "      <td>0.012941</td>\n",
       "      <td>...</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.989780</td>\n",
       "      <td>0.035754</td>\n",
       "      <td>AU</td>\n",
       "      <td>1.804126</td>\n",
       "      <td>3.113719</td>\n",
       "      <td>2.024285</td>\n",
       "      <td>0</td>\n",
       "      <td>0.636365</td>\n",
       "      <td>2.857144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.191265</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.301630</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.598896</td>\n",
       "      <td>AF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.957825</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.943877</td>\n",
       "      <td>5.310079</td>\n",
       "      <td>C</td>\n",
       "      <td>4.410969</td>\n",
       "      <td>5.326159</td>\n",
       "      <td>3.979592</td>\n",
       "      <td>3.928571</td>\n",
       "      <td>0.019645</td>\n",
       "      <td>...</td>\n",
       "      <td>9.333333</td>\n",
       "      <td>2.477596</td>\n",
       "      <td>0.013452</td>\n",
       "      <td>AE</td>\n",
       "      <td>1.773709</td>\n",
       "      <td>3.922193</td>\n",
       "      <td>1.120468</td>\n",
       "      <td>2</td>\n",
       "      <td>0.883118</td>\n",
       "      <td>1.176472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.797415</td>\n",
       "      <td>8.304757</td>\n",
       "      <td>C</td>\n",
       "      <td>4.225930</td>\n",
       "      <td>11.627438</td>\n",
       "      <td>2.097700</td>\n",
       "      <td>1.987549</td>\n",
       "      <td>0.171947</td>\n",
       "      <td>...</td>\n",
       "      <td>7.018256</td>\n",
       "      <td>1.812795</td>\n",
       "      <td>0.002267</td>\n",
       "      <td>CJ</td>\n",
       "      <td>1.415230</td>\n",
       "      <td>2.954381</td>\n",
       "      <td>1.990847</td>\n",
       "      <td>1</td>\n",
       "      <td>1.677108</td>\n",
       "      <td>1.034483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 133 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  target        v1        v2 v3        v4         v5        v6        v7  \\\n",
       "0   3       1  1.335739  8.727474  C  3.921026   7.915266  2.599278  3.176895   \n",
       "1   4       1       NaN       NaN  C       NaN   9.191265       NaN       NaN   \n",
       "2   5       1  0.943877  5.310079  C  4.410969   5.326159  3.979592  3.928571   \n",
       "3   6       1  0.797415  8.304757  C  4.225930  11.627438  2.097700  1.987549   \n",
       "4   8       1       NaN       NaN  C       NaN        NaN       NaN       NaN   \n",
       "\n",
       "         v8  ...      v122      v123      v124  v125      v126      v127  \\\n",
       "0  0.012941  ...  8.000000  1.989780  0.035754    AU  1.804126  3.113719   \n",
       "1  2.301630  ...       NaN       NaN  0.598896    AF       NaN       NaN   \n",
       "2  0.019645  ...  9.333333  2.477596  0.013452    AE  1.773709  3.922193   \n",
       "3  0.171947  ...  7.018256  1.812795  0.002267    CJ  1.415230  2.954381   \n",
       "4       NaN  ...       NaN       NaN       NaN     Z       NaN       NaN   \n",
       "\n",
       "       v128  v129      v130      v131  \n",
       "0  2.024285     0  0.636365  2.857144  \n",
       "1  1.957825     0       NaN       NaN  \n",
       "2  1.120468     2  0.883118  1.176472  \n",
       "3  1.990847     1  1.677108  1.034483  \n",
       "4       NaN     0       NaN       NaN  \n",
       "\n",
       "[5 rows x 133 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 114)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In practice, feature selection should be done after data pre-processing,\n",
    "# so ideally, all the categorical variables are encoded into numbers,\n",
    "# and then you can assess how deterministic they are of the target\n",
    "\n",
    "# here for simplicity I will use only numerical variables\n",
    "# select numerical columns:\n",
    "\n",
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "numerical_vars = list(data.select_dtypes(include=numerics).columns)\n",
    "data = data[numerical_vars]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important\n",
    "\n",
    "In all feature selection procedures, it is good practice to select the features by examining only the training set. And this is to avoid overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35000, 112), (15000, 112))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(labels=['target', 'ID'], axis=1),\n",
    "    data['target'],\n",
    "    test_size=0.3,\n",
    "    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test all features xgb ROC AUC=0.713140\n"
     ]
    }
   ],
   "source": [
    "# the first step of this procedure  consists in building\n",
    "# a machine learning algorithm using all the available features\n",
    "# and then determine the importance of the features according\n",
    "# to the algorithm\n",
    "\n",
    "# set the seed for reproducibility\n",
    "seed_val = 1000000000\n",
    "np.random.seed(seed_val)\n",
    "\n",
    "# build initial model using all the features\n",
    "model_all_features = xgb.XGBClassifier(\n",
    "    nthread=10, max_depth=4, n_estimators=500, learning_rate=0.05)\n",
    "\n",
    "model_all_features.fit(X_train, y_train)\n",
    "\n",
    "# calculate the roc-auc in the test set\n",
    "y_pred_test = model_all_features.predict_proba(X_test)[:, 1]\n",
    "auc_score_all = roc_auc_score(y_test, y_pred_test)\n",
    "print('Test all features xgb ROC AUC=%f' % (auc_score_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x13dc7400>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIcAAAF0CAYAAAC5T1mLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<Figure size 1440x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the second step consist of deriving the importance of \n",
    "# each feature and ranking them from the least to the most\n",
    "# important\n",
    "\n",
    "# get feature name and importance\n",
    "features = pd.Series(model_all_features.feature_importances_)\n",
    "features.index = X_train.columns\n",
    "\n",
    "# sort the features by importance\n",
    "features.sort_values(ascending=True, inplace=True)\n",
    "\n",
    "# plot\n",
    "features.plot.bar(figsize=(20,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['v48',\n",
       " 'v95',\n",
       " 'v76',\n",
       " 'v62',\n",
       " 'v100',\n",
       " 'v17',\n",
       " 'v64',\n",
       " 'v53',\n",
       " 'v128',\n",
       " 'v67',\n",
       " 'v94',\n",
       " 'v41',\n",
       " 'v96',\n",
       " 'v116',\n",
       " 'v60',\n",
       " 'v4',\n",
       " 'v104',\n",
       " 'v105',\n",
       " 'v63',\n",
       " 'v44',\n",
       " 'v109',\n",
       " 'v83',\n",
       " 'v13',\n",
       " 'v15',\n",
       " 'v51',\n",
       " 'v106',\n",
       " 'v84',\n",
       " 'v121',\n",
       " 'v115',\n",
       " 'v7',\n",
       " 'v8',\n",
       " 'v29',\n",
       " 'v46',\n",
       " 'v65',\n",
       " 'v25',\n",
       " 'v93',\n",
       " 'v20',\n",
       " 'v43',\n",
       " 'v33',\n",
       " 'v77',\n",
       " 'v58',\n",
       " 'v111',\n",
       " 'v89',\n",
       " 'v49',\n",
       " 'v26',\n",
       " 'v117',\n",
       " 'v108',\n",
       " 'v101',\n",
       " 'v61',\n",
       " 'v70',\n",
       " 'v118',\n",
       " 'v54',\n",
       " 'v81',\n",
       " 'v2',\n",
       " 'v37',\n",
       " 'v88',\n",
       " 'v98',\n",
       " 'v90',\n",
       " 'v11',\n",
       " 'v119',\n",
       " 'v103',\n",
       " 'v68',\n",
       " 'v9',\n",
       " 'v27',\n",
       " 'v59',\n",
       " 'v5',\n",
       " 'v120',\n",
       " 'v1',\n",
       " 'v97',\n",
       " 'v36',\n",
       " 'v126',\n",
       " 'v32',\n",
       " 'v28',\n",
       " 'v130',\n",
       " 'v86',\n",
       " 'v45',\n",
       " 'v80',\n",
       " 'v122',\n",
       " 'v35',\n",
       " 'v55',\n",
       " 'v92',\n",
       " 'v78',\n",
       " 'v19',\n",
       " 'v124',\n",
       " 'v72',\n",
       " 'v73',\n",
       " 'v82',\n",
       " 'v131',\n",
       " 'v85',\n",
       " 'v123',\n",
       " 'v127',\n",
       " 'v18',\n",
       " 'v57',\n",
       " 'v38',\n",
       " 'v42',\n",
       " 'v16',\n",
       " 'v39',\n",
       " 'v87',\n",
       " 'v102',\n",
       " 'v69',\n",
       " 'v6',\n",
       " 'v99',\n",
       " 'v129',\n",
       " 'v23',\n",
       " 'v40',\n",
       " 'v14',\n",
       " 'v34',\n",
       " 'v12',\n",
       " 'v114',\n",
       " 'v21',\n",
       " 'v10',\n",
       " 'v50']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view the list of ordered features\n",
    "features = list(features.index)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing recursive feature elimination\n",
      "\n",
      "testing feature:  v48  which is feature  1  out of  112\n",
      "New Test ROC AUC=0.7127500791811988\n",
      "All features Test ROC AUC=0.7131402642043081\n",
      "Drop in ROC AUC=0.0003901850231092796\n",
      "remove:  v48\n",
      "\n",
      "testing feature:  v95  which is feature  2  out of  112\n",
      "New Test ROC AUC=0.7135813338128427\n",
      "All features Test ROC AUC=0.7127500791811988\n",
      "Drop in ROC AUC=-0.0008312546316439073\n",
      "remove:  v95\n",
      "\n",
      "testing feature:  v76  which is feature  3  out of  112\n",
      "New Test ROC AUC=0.7138054503888731\n",
      "All features Test ROC AUC=0.7135813338128427\n",
      "Drop in ROC AUC=-0.00022411657603038648\n",
      "remove:  v76\n",
      "\n",
      "testing feature:  v62  which is feature  4  out of  112\n",
      "New Test ROC AUC=0.7136766175725151\n",
      "All features Test ROC AUC=0.7138054503888731\n",
      "Drop in ROC AUC=0.00012883281635800437\n",
      "remove:  v62\n",
      "\n",
      "testing feature:  v100  which is feature  5  out of  112\n",
      "New Test ROC AUC=0.7133000075260361\n",
      "All features Test ROC AUC=0.7136766175725151\n",
      "Drop in ROC AUC=0.0003766100464790423\n",
      "remove:  v100\n",
      "\n",
      "testing feature:  v17  which is feature  6  out of  112\n",
      "New Test ROC AUC=0.7133915677226363\n",
      "All features Test ROC AUC=0.7133000075260361\n",
      "Drop in ROC AUC=-9.156019660028214e-05\n",
      "remove:  v17\n",
      "\n",
      "testing feature:  v64  which is feature  7  out of  112\n",
      "New Test ROC AUC=0.7142117996212126\n",
      "All features Test ROC AUC=0.7133915677226363\n",
      "Drop in ROC AUC=-0.0008202318985762735\n",
      "remove:  v64\n",
      "\n",
      "testing feature:  v53  which is feature  8  out of  112\n",
      "New Test ROC AUC=0.714460279643039\n",
      "All features Test ROC AUC=0.7142117996212126\n",
      "Drop in ROC AUC=-0.0002484800218264249\n",
      "remove:  v53\n",
      "\n",
      "testing feature:  v128  which is feature  9  out of  112\n",
      "New Test ROC AUC=0.7140068433664206\n",
      "All features Test ROC AUC=0.714460279643039\n",
      "Drop in ROC AUC=0.0004534362766184241\n",
      "remove:  v128\n",
      "\n",
      "testing feature:  v67  which is feature  10  out of  112\n",
      "New Test ROC AUC=0.7138425873821614\n",
      "All features Test ROC AUC=0.7140068433664206\n",
      "Drop in ROC AUC=0.0001642559842591762\n",
      "remove:  v67\n",
      "\n",
      "testing feature:  v94  which is feature  11  out of  112\n",
      "New Test ROC AUC=0.7131230766317171\n",
      "All features Test ROC AUC=0.7138425873821614\n",
      "Drop in ROC AUC=0.0007195107504442966\n",
      "keep:  v94\n",
      "\n",
      "testing feature:  v41  which is feature  12  out of  112\n",
      "New Test ROC AUC=0.7138039091789923\n",
      "All features Test ROC AUC=0.7138425873821614\n",
      "Drop in ROC AUC=3.867820316916237e-05\n",
      "remove:  v41\n",
      "\n",
      "testing feature:  v96  which is feature  13  out of  112\n",
      "New Test ROC AUC=0.7136803164762289\n",
      "All features Test ROC AUC=0.7138039091789923\n",
      "Drop in ROC AUC=0.00012359270276340872\n",
      "remove:  v96\n",
      "\n",
      "testing feature:  v116  which is feature  14  out of  112\n",
      "New Test ROC AUC=0.7144623756884769\n",
      "All features Test ROC AUC=0.7136803164762289\n",
      "Drop in ROC AUC=-0.0007820592122480541\n",
      "remove:  v116\n",
      "\n",
      "testing feature:  v60  which is feature  15  out of  112\n",
      "New Test ROC AUC=0.7132133545416964\n",
      "All features Test ROC AUC=0.7144623756884769\n",
      "Drop in ROC AUC=0.0012490211467804935\n",
      "keep:  v60\n",
      "\n",
      "testing feature:  v4  which is feature  16  out of  112\n",
      "New Test ROC AUC=0.7138798846612776\n",
      "All features Test ROC AUC=0.7144623756884769\n",
      "Drop in ROC AUC=0.0005824910271993167\n",
      "keep:  v4\n",
      "\n",
      "testing feature:  v104  which is feature  17  out of  112\n",
      "New Test ROC AUC=0.7143199309064513\n",
      "All features Test ROC AUC=0.7144623756884769\n",
      "Drop in ROC AUC=0.00014244478202563027\n",
      "remove:  v104\n",
      "\n",
      "testing feature:  v105  which is feature  18  out of  112\n",
      "New Test ROC AUC=0.7133649972642908\n",
      "All features Test ROC AUC=0.7143199309064513\n",
      "Drop in ROC AUC=0.0009549336421604826\n",
      "keep:  v105\n",
      "\n",
      "testing feature:  v63  which is feature  19  out of  112\n",
      "New Test ROC AUC=0.7141942914769663\n",
      "All features Test ROC AUC=0.7143199309064513\n",
      "Drop in ROC AUC=0.00012563942948495832\n",
      "remove:  v63\n",
      "\n",
      "testing feature:  v44  which is feature  20  out of  112\n",
      "New Test ROC AUC=0.7126713911695233\n",
      "All features Test ROC AUC=0.7141942914769663\n",
      "Drop in ROC AUC=0.0015229003074430647\n",
      "keep:  v44\n",
      "\n",
      "testing feature:  v109  which is feature  21  out of  112\n",
      "New Test ROC AUC=0.7148035379076969\n",
      "All features Test ROC AUC=0.7141942914769663\n",
      "Drop in ROC AUC=-0.0006092464307305701\n",
      "remove:  v109\n",
      "\n",
      "testing feature:  v83  which is feature  22  out of  112\n",
      "New Test ROC AUC=0.714305024324484\n",
      "All features Test ROC AUC=0.7148035379076969\n",
      "Drop in ROC AUC=0.0004985135832129561\n",
      "remove:  v83\n",
      "\n",
      "testing feature:  v13  which is feature  23  out of  112\n",
      "New Test ROC AUC=0.71330047605384\n",
      "All features Test ROC AUC=0.714305024324484\n",
      "Drop in ROC AUC=0.0010045482706440012\n",
      "keep:  v13\n",
      "\n",
      "testing feature:  v15  which is feature  24  out of  112\n",
      "New Test ROC AUC=0.7137441102356162\n",
      "All features Test ROC AUC=0.714305024324484\n",
      "Drop in ROC AUC=0.0005609140888677144\n",
      "keep:  v15\n",
      "\n",
      "testing feature:  v51  which is feature  25  out of  112\n",
      "New Test ROC AUC=0.7143617161887403\n",
      "All features Test ROC AUC=0.714305024324484\n",
      "Drop in ROC AUC=-5.669186425638628e-05\n",
      "remove:  v51\n",
      "\n",
      "testing feature:  v106  which is feature  26  out of  112\n",
      "New Test ROC AUC=0.713621343621349\n",
      "All features Test ROC AUC=0.7143617161887403\n",
      "Drop in ROC AUC=0.0007403725673913453\n",
      "keep:  v106\n",
      "\n",
      "testing feature:  v84  which is feature  27  out of  112\n",
      "New Test ROC AUC=0.7143414338667087\n",
      "All features Test ROC AUC=0.7143617161887403\n",
      "Drop in ROC AUC=2.0282322031661693e-05\n",
      "remove:  v84\n",
      "\n",
      "testing feature:  v121  which is feature  28  out of  112\n",
      "New Test ROC AUC=0.7132296543773962\n",
      "All features Test ROC AUC=0.7143414338667087\n",
      "Drop in ROC AUC=0.0011117794893125144\n",
      "keep:  v121\n",
      "\n",
      "testing feature:  v115  which is feature  29  out of  112\n",
      "New Test ROC AUC=0.7134980098418457\n",
      "All features Test ROC AUC=0.7143414338667087\n",
      "Drop in ROC AUC=0.0008434240248629266\n",
      "keep:  v115\n",
      "\n",
      "testing feature:  v7  which is feature  30  out of  112\n",
      "New Test ROC AUC=0.7141579929018531\n",
      "All features Test ROC AUC=0.7143414338667087\n",
      "Drop in ROC AUC=0.0001834409648555635\n",
      "remove:  v7\n",
      "\n",
      "testing feature:  v8  which is feature  31  out of  112\n",
      "New Test ROC AUC=0.7145207197297255\n",
      "All features Test ROC AUC=0.7141579929018531\n",
      "Drop in ROC AUC=-0.00036272682787241717\n",
      "remove:  v8\n",
      "\n",
      "testing feature:  v29  which is feature  32  out of  112\n",
      "New Test ROC AUC=0.7132763222125875\n",
      "All features Test ROC AUC=0.7145207197297255\n",
      "Drop in ROC AUC=0.0012443975171380073\n",
      "keep:  v29\n",
      "\n",
      "testing feature:  v46  which is feature  33  out of  112\n",
      "New Test ROC AUC=0.7145484615075804\n",
      "All features Test ROC AUC=0.7145207197297255\n",
      "Drop in ROC AUC=-2.774177785491716e-05\n",
      "remove:  v46\n",
      "\n",
      "testing feature:  v65  which is feature  34  out of  112\n",
      "New Test ROC AUC=0.7139297582130212\n",
      "All features Test ROC AUC=0.7145484615075804\n",
      "Drop in ROC AUC=0.0006187032945592641\n",
      "keep:  v65\n",
      "\n",
      "testing feature:  v25  which is feature  35  out of  112\n",
      "New Test ROC AUC=0.714004315782216\n",
      "All features Test ROC AUC=0.7145484615075804\n",
      "Drop in ROC AUC=0.0005441457253644311\n",
      "keep:  v25\n",
      "\n",
      "testing feature:  v93  which is feature  36  out of  112\n",
      "New Test ROC AUC=0.714028099733097\n",
      "All features Test ROC AUC=0.7145484615075804\n",
      "Drop in ROC AUC=0.0005203617744834466\n",
      "keep:  v93\n",
      "\n",
      "testing feature:  v20  which is feature  37  out of  112\n",
      "New Test ROC AUC=0.7141866224165992\n",
      "All features Test ROC AUC=0.7145484615075804\n",
      "Drop in ROC AUC=0.0003618390909811975\n",
      "remove:  v20\n",
      "\n",
      "testing feature:  v43  which is feature  38  out of  112\n",
      "New Test ROC AUC=0.7144341037344231\n",
      "All features Test ROC AUC=0.7141866224165992\n",
      "Drop in ROC AUC=-0.00024748131782381666\n",
      "remove:  v43\n",
      "\n",
      "testing feature:  v33  which is feature  39  out of  112\n",
      "New Test ROC AUC=0.714651229382434\n",
      "All features Test ROC AUC=0.7144341037344231\n",
      "Drop in ROC AUC=-0.0002171256480109074\n",
      "remove:  v33\n",
      "\n",
      "testing feature:  v77  which is feature  40  out of  112\n",
      "New Test ROC AUC=0.7132074609551121\n",
      "All features Test ROC AUC=0.714651229382434\n",
      "Drop in ROC AUC=0.0014437684273218565\n",
      "keep:  v77\n",
      "\n",
      "testing feature:  v58  which is feature  41  out of  112\n",
      "New Test ROC AUC=0.7132540301528713\n",
      "All features Test ROC AUC=0.714651229382434\n",
      "Drop in ROC AUC=0.0013971992295627178\n",
      "keep:  v58\n",
      "\n",
      "testing feature:  v111  which is feature  42  out of  112\n",
      "New Test ROC AUC=0.7133901004908297\n",
      "All features Test ROC AUC=0.714651229382434\n",
      "Drop in ROC AUC=0.0012611288916042351\n",
      "keep:  v111\n",
      "\n",
      "testing feature:  v89  which is feature  43  out of  112\n",
      "New Test ROC AUC=0.7141664880507163\n",
      "All features Test ROC AUC=0.714651229382434\n",
      "Drop in ROC AUC=0.0004847413317177196\n",
      "remove:  v89\n",
      "\n",
      "testing feature:  v49  which is feature  44  out of  112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Test ROC AUC=0.7142214907489433\n",
      "All features Test ROC AUC=0.7141664880507163\n",
      "Drop in ROC AUC=-5.500269822700243e-05\n",
      "remove:  v49\n",
      "\n",
      "testing feature:  v26  which is feature  45  out of  112\n",
      "New Test ROC AUC=0.7135912468747962\n",
      "All features Test ROC AUC=0.7142214907489433\n",
      "Drop in ROC AUC=0.0006302438741470073\n",
      "keep:  v26\n",
      "\n",
      "testing feature:  v117  which is feature  46  out of  112\n",
      "New Test ROC AUC=0.7140071762677549\n",
      "All features Test ROC AUC=0.7142214907489433\n",
      "Drop in ROC AUC=0.00021431448118836016\n",
      "remove:  v117\n",
      "\n",
      "testing feature:  v108  which is feature  47  out of  112\n",
      "New Test ROC AUC=0.7143510633460441\n",
      "All features Test ROC AUC=0.7140071762677549\n",
      "Drop in ROC AUC=-0.00034388707828925114\n",
      "remove:  v108\n",
      "\n",
      "testing feature:  v101  which is feature  48  out of  112\n",
      "New Test ROC AUC=0.712959856340498\n",
      "All features Test ROC AUC=0.7143510633460441\n",
      "Drop in ROC AUC=0.00139120700554618\n",
      "keep:  v101\n",
      "\n",
      "testing feature:  v61  which is feature  49  out of  112\n",
      "New Test ROC AUC=0.7140325260878748\n",
      "All features Test ROC AUC=0.7143510633460441\n",
      "Drop in ROC AUC=0.0003185372581693269\n",
      "remove:  v61\n",
      "\n",
      "testing feature:  v70  which is feature  50  out of  112\n",
      "New Test ROC AUC=0.7142343382745097\n",
      "All features Test ROC AUC=0.7140325260878748\n",
      "Drop in ROC AUC=-0.00020181218663484213\n",
      "remove:  v70\n",
      "\n",
      "testing feature:  v118  which is feature  51  out of  112\n",
      "New Test ROC AUC=0.713801714496122\n",
      "All features Test ROC AUC=0.7142343382745097\n",
      "Drop in ROC AUC=0.0004326237783877085\n",
      "remove:  v118\n",
      "\n",
      "testing feature:  v54  which is feature  52  out of  112\n",
      "New Test ROC AUC=0.7135883370705414\n",
      "All features Test ROC AUC=0.713801714496122\n",
      "Drop in ROC AUC=0.00021337742558058537\n",
      "remove:  v54\n",
      "\n",
      "testing feature:  v81  which is feature  53  out of  112\n",
      "New Test ROC AUC=0.7142288762266922\n",
      "All features Test ROC AUC=0.7135883370705414\n",
      "Drop in ROC AUC=-0.00064053915615081\n",
      "remove:  v81\n",
      "\n",
      "testing feature:  v2  which is feature  54  out of  112\n",
      "New Test ROC AUC=0.7138526114112266\n",
      "All features Test ROC AUC=0.7142288762266922\n",
      "Drop in ROC AUC=0.000376264815465599\n",
      "remove:  v2\n",
      "\n",
      "testing feature:  v37  which is feature  55  out of  112\n",
      "New Test ROC AUC=0.7133952912857082\n",
      "All features Test ROC AUC=0.7138526114112266\n",
      "Drop in ROC AUC=0.00045732012551835677\n",
      "remove:  v37\n",
      "\n",
      "testing feature:  v88  which is feature  56  out of  112\n",
      "New Test ROC AUC=0.7131073686206117\n",
      "All features Test ROC AUC=0.7133952912857082\n",
      "Drop in ROC AUC=0.0002879226650964739\n",
      "remove:  v88\n",
      "\n",
      "testing feature:  v98  which is feature  57  out of  112\n",
      "New Test ROC AUC=0.7137241114962026\n",
      "All features Test ROC AUC=0.7131073686206117\n",
      "Drop in ROC AUC=-0.000616742875590881\n",
      "remove:  v98\n",
      "\n",
      "testing feature:  v90  which is feature  58  out of  112\n",
      "New Test ROC AUC=0.7136833002585583\n",
      "All features Test ROC AUC=0.7137241114962026\n",
      "Drop in ROC AUC=4.0811237644322595e-05\n",
      "remove:  v90\n",
      "\n",
      "testing feature:  v11  which is feature  59  out of  112\n",
      "New Test ROC AUC=0.7137953893707711\n",
      "All features Test ROC AUC=0.7136833002585583\n",
      "Drop in ROC AUC=-0.00011208911221283202\n",
      "remove:  v11\n",
      "\n",
      "testing feature:  v119  which is feature  60  out of  112\n",
      "New Test ROC AUC=0.7145097586450532\n",
      "All features Test ROC AUC=0.7137953893707711\n",
      "Drop in ROC AUC=-0.000714369274282034\n",
      "remove:  v119\n",
      "\n",
      "testing feature:  v103  which is feature  61  out of  112\n",
      "New Test ROC AUC=0.7144785398977069\n",
      "All features Test ROC AUC=0.7145097586450532\n",
      "Drop in ROC AUC=3.121874734623997e-05\n",
      "remove:  v103\n",
      "\n",
      "testing feature:  v68  which is feature  62  out of  112\n",
      "New Test ROC AUC=0.7144543490674176\n",
      "All features Test ROC AUC=0.7144785398977069\n",
      "Drop in ROC AUC=2.41908302893723e-05\n",
      "remove:  v68\n",
      "\n",
      "testing feature:  v9  which is feature  63  out of  112\n",
      "New Test ROC AUC=0.7136223423253518\n",
      "All features Test ROC AUC=0.7144543490674176\n",
      "Drop in ROC AUC=0.0008320067420657384\n",
      "keep:  v9\n",
      "\n",
      "testing feature:  v27  which is feature  64  out of  112\n",
      "New Test ROC AUC=0.7146013188416533\n",
      "All features Test ROC AUC=0.7144543490674176\n",
      "Drop in ROC AUC=-0.0001469697742357834\n",
      "remove:  v27\n",
      "\n",
      "testing feature:  v59  which is feature  65  out of  112\n",
      "New Test ROC AUC=0.7137409291784222\n",
      "All features Test ROC AUC=0.7146013188416533\n",
      "Drop in ROC AUC=0.0008603896632310981\n",
      "keep:  v59\n",
      "\n",
      "testing feature:  v5  which is feature  66  out of  112\n",
      "New Test ROC AUC=0.7139802975674332\n",
      "All features Test ROC AUC=0.7146013188416533\n",
      "Drop in ROC AUC=0.000621021274220146\n",
      "keep:  v5\n",
      "\n",
      "testing feature:  v120  which is feature  67  out of  112\n",
      "New Test ROC AUC=0.7154292074709175\n",
      "All features Test ROC AUC=0.7146013188416533\n",
      "Drop in ROC AUC=-0.0008278886292641952\n",
      "remove:  v120\n",
      "\n",
      "testing feature:  v1  which is feature  68  out of  112\n",
      "New Test ROC AUC=0.7159605426597524\n",
      "All features Test ROC AUC=0.7154292074709175\n",
      "Drop in ROC AUC=-0.0005313351888348583\n",
      "remove:  v1\n",
      "\n",
      "testing feature:  v97  which is feature  69  out of  112\n",
      "New Test ROC AUC=0.7161673730257594\n",
      "All features Test ROC AUC=0.7159605426597524\n",
      "Drop in ROC AUC=-0.00020683036600699367\n",
      "remove:  v97\n",
      "\n",
      "testing feature:  v36  which is feature  70  out of  112\n",
      "New Test ROC AUC=0.7151118538619365\n",
      "All features Test ROC AUC=0.7161673730257594\n",
      "Drop in ROC AUC=0.001055519163822849\n",
      "keep:  v36\n",
      "\n",
      "testing feature:  v126  which is feature  71  out of  112\n",
      "New Test ROC AUC=0.7154339913863875\n",
      "All features Test ROC AUC=0.7161673730257594\n",
      "Drop in ROC AUC=0.0007333816393718662\n",
      "keep:  v126\n",
      "\n",
      "testing feature:  v32  which is feature  72  out of  112\n",
      "New Test ROC AUC=0.7147766592073752\n",
      "All features Test ROC AUC=0.7161673730257594\n",
      "Drop in ROC AUC=0.0013907138183841816\n",
      "keep:  v32\n",
      "\n",
      "testing feature:  v28  which is feature  73  out of  112\n",
      "New Test ROC AUC=0.7156326841642238\n",
      "All features Test ROC AUC=0.7161673730257594\n",
      "Drop in ROC AUC=0.000534688861535626\n",
      "keep:  v28\n",
      "\n",
      "testing feature:  v130  which is feature  74  out of  112\n",
      "New Test ROC AUC=0.714935009275371\n",
      "All features Test ROC AUC=0.7161673730257594\n",
      "Drop in ROC AUC=0.0012323637503883766\n",
      "keep:  v130\n",
      "\n",
      "testing feature:  v86  which is feature  75  out of  112\n",
      "New Test ROC AUC=0.7148376417999398\n",
      "All features Test ROC AUC=0.7161673730257594\n",
      "Drop in ROC AUC=0.001329731225819586\n",
      "keep:  v86\n",
      "\n",
      "testing feature:  v45  which is feature  76  out of  112\n",
      "New Test ROC AUC=0.7141124470674549\n",
      "All features Test ROC AUC=0.7161673730257594\n",
      "Drop in ROC AUC=0.0020549259583044766\n",
      "keep:  v45\n",
      "\n",
      "testing feature:  v80  which is feature  77  out of  112\n",
      "New Test ROC AUC=0.7153748829050381\n",
      "All features Test ROC AUC=0.7161673730257594\n",
      "Drop in ROC AUC=0.0007924901207212454\n",
      "keep:  v80\n",
      "\n",
      "testing feature:  v122  which is feature  78  out of  112\n",
      "New Test ROC AUC=0.7150437817039202\n",
      "All features Test ROC AUC=0.7161673730257594\n",
      "Drop in ROC AUC=0.0011235913218391458\n",
      "keep:  v122\n",
      "\n",
      "testing feature:  v35  which is feature  79  out of  112\n",
      "New Test ROC AUC=0.7154375916526692\n",
      "All features Test ROC AUC=0.7161673730257594\n",
      "Drop in ROC AUC=0.0007297813730902103\n",
      "keep:  v35\n",
      "\n",
      "testing feature:  v55  which is feature  80  out of  112\n",
      "New Test ROC AUC=0.7155505685017733\n",
      "All features Test ROC AUC=0.7161673730257594\n",
      "Drop in ROC AUC=0.0006168045239860476\n",
      "keep:  v55\n",
      "\n",
      "testing feature:  v92  which is feature  81  out of  112\n",
      "New Test ROC AUC=0.7156116374020913\n",
      "All features Test ROC AUC=0.7161673730257594\n",
      "Drop in ROC AUC=0.0005557356236680633\n",
      "keep:  v92\n",
      "\n",
      "testing feature:  v78  which is feature  82  out of  112\n",
      "New Test ROC AUC=0.7151442562584711\n",
      "All features Test ROC AUC=0.7161673730257594\n",
      "Drop in ROC AUC=0.001023116767288279\n",
      "keep:  v78\n",
      "\n",
      "testing feature:  v19  which is feature  83  out of  112\n",
      "New Test ROC AUC=0.7152726452063836\n",
      "All features Test ROC AUC=0.7161673730257594\n",
      "Drop in ROC AUC=0.000894727819375829\n",
      "keep:  v19\n",
      "\n",
      "testing feature:  v124  which is feature  84  out of  112\n",
      "New Test ROC AUC=0.7148569377476477\n",
      "All features Test ROC AUC=0.7161673730257594\n",
      "Drop in ROC AUC=0.001310435278111699\n",
      "keep:  v124\n",
      "\n",
      "testing feature:  v72  which is feature  85  out of  112\n",
      "New Test ROC AUC=0.7149492377249907\n",
      "All features Test ROC AUC=0.7161673730257594\n",
      "Drop in ROC AUC=0.0012181353007686413\n",
      "keep:  v72\n",
      "\n",
      "testing feature:  v73  which is feature  86  out of  112\n",
      "New Test ROC AUC=0.7149398178501992\n",
      "All features Test ROC AUC=0.7161673730257594\n",
      "Drop in ROC AUC=0.0012275551755601688\n",
      "keep:  v73\n",
      "\n",
      "testing feature:  v82  which is feature  87  out of  112\n",
      "New Test ROC AUC=0.7156209709691295\n",
      "All features Test ROC AUC=0.7161673730257594\n",
      "Drop in ROC AUC=0.0005464020566299244\n",
      "keep:  v82\n",
      "\n",
      "testing feature:  v131  which is feature  88  out of  112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Test ROC AUC=0.7158320550744075\n",
      "All features Test ROC AUC=0.7161673730257594\n",
      "Drop in ROC AUC=0.0003353179513518878\n",
      "remove:  v131\n",
      "\n",
      "testing feature:  v85  which is feature  89  out of  112\n",
      "New Test ROC AUC=0.7149654882419743\n",
      "All features Test ROC AUC=0.7158320550744075\n",
      "Drop in ROC AUC=0.0008665668324332465\n",
      "keep:  v85\n",
      "\n",
      "testing feature:  v123  which is feature  90  out of  112\n",
      "New Test ROC AUC=0.7150391950633148\n",
      "All features Test ROC AUC=0.7158320550744075\n",
      "Drop in ROC AUC=0.0007928600110926887\n",
      "keep:  v123\n",
      "\n",
      "testing feature:  v127  which is feature  91  out of  112\n",
      "New Test ROC AUC=0.7151325430633768\n",
      "All features Test ROC AUC=0.7158320550744075\n",
      "Drop in ROC AUC=0.0006995120110306896\n",
      "keep:  v127\n",
      "\n",
      "testing feature:  v18  which is feature  92  out of  112\n",
      "New Test ROC AUC=0.7147288817010695\n",
      "All features Test ROC AUC=0.7158320550744075\n",
      "Drop in ROC AUC=0.0011031733733379845\n",
      "keep:  v18\n",
      "\n",
      "testing feature:  v57  which is feature  93  out of  112\n",
      "New Test ROC AUC=0.7154464320325455\n",
      "All features Test ROC AUC=0.7158320550744075\n",
      "Drop in ROC AUC=0.00038562304186195995\n",
      "remove:  v57\n",
      "\n",
      "testing feature:  v38  which is feature  94  out of  112\n",
      "New Test ROC AUC=0.7142176069000434\n",
      "All features Test ROC AUC=0.7154464320325455\n",
      "Drop in ROC AUC=0.0012288251325021093\n",
      "keep:  v38\n",
      "\n",
      "testing feature:  v42  which is feature  95  out of  112\n",
      "New Test ROC AUC=0.7152375302804588\n",
      "All features Test ROC AUC=0.7154464320325455\n",
      "Drop in ROC AUC=0.0002089017520867653\n",
      "remove:  v42\n",
      "\n",
      "testing feature:  v16  which is feature  96  out of  112\n",
      "New Test ROC AUC=0.7150824968961267\n",
      "All features Test ROC AUC=0.7152375302804588\n",
      "Drop in ROC AUC=0.0001550333843320928\n",
      "remove:  v16\n",
      "\n",
      "testing feature:  v39  which is feature  97  out of  112\n",
      "New Test ROC AUC=0.7155844751191516\n",
      "All features Test ROC AUC=0.7150824968961267\n",
      "Drop in ROC AUC=-0.0005019782230248904\n",
      "remove:  v39\n",
      "\n",
      "testing feature:  v87  which is feature  98  out of  112\n",
      "New Test ROC AUC=0.716152971960633\n",
      "All features Test ROC AUC=0.7155844751191516\n",
      "Drop in ROC AUC=-0.000568496841481414\n",
      "remove:  v87\n",
      "\n",
      "testing feature:  v102  which is feature  99  out of  112\n",
      "New Test ROC AUC=0.7161611342081619\n",
      "All features Test ROC AUC=0.716152971960633\n",
      "Drop in ROC AUC=-8.162247528864519e-06\n",
      "remove:  v102\n",
      "\n",
      "testing feature:  v69  which is feature  100  out of  112\n",
      "New Test ROC AUC=0.7163049969032778\n",
      "All features Test ROC AUC=0.7161611342081619\n",
      "Drop in ROC AUC=-0.00014386269511590388\n",
      "remove:  v69\n",
      "\n",
      "testing feature:  v6  which is feature  101  out of  112\n",
      "New Test ROC AUC=0.7149546874431294\n",
      "All features Test ROC AUC=0.7163049969032778\n",
      "Drop in ROC AUC=0.0013503094601483578\n",
      "keep:  v6\n",
      "\n",
      "testing feature:  v99  which is feature  102  out of  112\n",
      "New Test ROC AUC=0.715268514763903\n",
      "All features Test ROC AUC=0.7163049969032778\n",
      "Drop in ROC AUC=0.0010364821393747947\n",
      "keep:  v99\n",
      "\n",
      "testing feature:  v129  which is feature  103  out of  112\n",
      "New Test ROC AUC=0.7130239953308986\n",
      "All features Test ROC AUC=0.7163049969032778\n",
      "Drop in ROC AUC=0.003281001572379205\n",
      "keep:  v129\n",
      "\n",
      "testing feature:  v23  which is feature  104  out of  112\n",
      "New Test ROC AUC=0.7163025309674684\n",
      "All features Test ROC AUC=0.7163049969032778\n",
      "Drop in ROC AUC=2.4659358093259698e-06\n",
      "remove:  v23\n",
      "\n",
      "testing feature:  v40  which is feature  105  out of  112\n",
      "New Test ROC AUC=0.7151963491524431\n",
      "All features Test ROC AUC=0.7163025309674684\n",
      "Drop in ROC AUC=0.0011061818150253089\n",
      "keep:  v40\n",
      "\n",
      "testing feature:  v14  which is feature  106  out of  112\n",
      "New Test ROC AUC=0.7146294798285957\n",
      "All features Test ROC AUC=0.7163025309674684\n",
      "Drop in ROC AUC=0.0016730511388727276\n",
      "keep:  v14\n",
      "\n",
      "testing feature:  v34  which is feature  107  out of  112\n",
      "New Test ROC AUC=0.7149052577598315\n",
      "All features Test ROC AUC=0.7163025309674684\n",
      "Drop in ROC AUC=0.0013972732076369399\n",
      "keep:  v34\n",
      "\n",
      "testing feature:  v12  which is feature  108  out of  112\n",
      "New Test ROC AUC=0.7147862270383154\n",
      "All features Test ROC AUC=0.7163025309674684\n",
      "Drop in ROC AUC=0.001516303929153029\n",
      "keep:  v12\n",
      "\n",
      "testing feature:  v114  which is feature  109  out of  112\n",
      "New Test ROC AUC=0.7147536027075581\n",
      "All features Test ROC AUC=0.7163025309674684\n",
      "Drop in ROC AUC=0.001548928259910376\n",
      "keep:  v114\n",
      "\n",
      "testing feature:  v21  which is feature  110  out of  112\n",
      "New Test ROC AUC=0.7144349668119563\n",
      "All features Test ROC AUC=0.7163025309674684\n",
      "Drop in ROC AUC=0.001867564155512147\n",
      "keep:  v21\n",
      "\n",
      "testing feature:  v10  which is feature  111  out of  112\n",
      "New Test ROC AUC=0.7078630875942813\n",
      "All features Test ROC AUC=0.7163025309674684\n",
      "Drop in ROC AUC=0.008439443373187139\n",
      "keep:  v10\n",
      "\n",
      "testing feature:  v50  which is feature  112  out of  112\n",
      "New Test ROC AUC=0.7127018331470893\n",
      "All features Test ROC AUC=0.7163025309674684\n",
      "Drop in ROC AUC=0.003600697820379084\n",
      "keep:  v50\n",
      "DONE!!\n",
      "total features to remove:  56\n",
      "total features to keep:  56\n"
     ]
    }
   ],
   "source": [
    "# the final step consists in removing one at a time\n",
    "# all the features, from the least to the most\n",
    "# important, and build an xgboost at each round.\n",
    "\n",
    "# once we build the model, we calculate the new roc-auc\n",
    "# if the new roc-auc is smaller than the original one\n",
    "# (with all the features), then that feature that was removed\n",
    "# was important, and we should keep it.\n",
    "# otherwise, we should remove the feature\n",
    "\n",
    "# recursive feature elimination:\n",
    "\n",
    "# first we arbitrarily set the drop in roc-auc\n",
    "# if the drop is below this threshold,\n",
    "# the feature will be removed\n",
    "tol = 0.0005\n",
    "\n",
    "print('doing recursive feature elimination')\n",
    "\n",
    "# we initialise a list where we will collect the\n",
    "# features we should remove\n",
    "features_to_remove = []\n",
    "\n",
    "# set a counter to know how far ahead the loop is going\n",
    "count = 1\n",
    "\n",
    "# now we loop over all the features, in order of importance:\n",
    "# remember that features is the list of ordered features\n",
    "# by importance\n",
    "for feature in features:\n",
    "    print()\n",
    "    print('testing feature: ', feature, ' which is feature ', count,\n",
    "          ' out of ', len(features))\n",
    "    count = count + 1\n",
    "\n",
    "    # initialise model\n",
    "    model_int = xgb.XGBClassifier(\n",
    "        nthread=10, max_depth=4, n_estimators=500, learning_rate=0.05)\n",
    "\n",
    "    # fit model with all variables minus the removed features\n",
    "    # and the feature to be evaluated\n",
    "    model_int.fit(\n",
    "        X_train.drop(features_to_remove + [feature], axis=1), y_train)\n",
    "\n",
    "    # make a prediction over the test set\n",
    "    y_pred_test = model_int.predict_proba(\n",
    "        X_test.drop(features_to_remove + [feature], axis=1))[:, 1]\n",
    "\n",
    "    # calculate the new roc-auc\n",
    "    auc_score_int = roc_auc_score(y_test, y_pred_test)\n",
    "    print('New Test ROC AUC={}'.format((auc_score_int)))\n",
    "\n",
    "    # print the original roc-auc with all the features\n",
    "    print('All features Test ROC AUC={}'.format((auc_score_all)))\n",
    "\n",
    "    # determine the drop in the roc-auc\n",
    "    diff_auc = auc_score_all - auc_score_int\n",
    "\n",
    "    # compare the drop in roc-auc with the tolerance\n",
    "    # we set previously\n",
    "    if diff_auc >= tol:\n",
    "        print('Drop in ROC AUC={}'.format(diff_auc))\n",
    "        print('keep: ', feature)\n",
    "        print\n",
    "    else:\n",
    "        print('Drop in ROC AUC={}'.format(diff_auc))\n",
    "        print('remove: ', feature)\n",
    "        print\n",
    "        # if the drop in the roc is small and we remove the\n",
    "        # feature, we need to set the new roc to the one based on\n",
    "        # the remaining features\n",
    "        auc_score_all = auc_score_int\n",
    "        \n",
    "        # and append the feature to remove to the collecting\n",
    "        # list\n",
    "        features_to_remove.append(feature)\n",
    "\n",
    "# now the loop is finished, we evaluated all the features\n",
    "print('DONE!!')\n",
    "print('total features to remove: ', len(features_to_remove))\n",
    "\n",
    "# determine the features to keep (those we won't remove)\n",
    "features_to_keep = [x for x in features if x not in features_to_remove]\n",
    "print('total features to keep: ', len(features_to_keep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test selected features ROC AUC=0.715491\n"
     ]
    }
   ],
   "source": [
    "# capture the 56 selected features\n",
    "seed_val = 1000000000\n",
    "np.random.seed(seed_val)\n",
    "\n",
    "# build initial model\n",
    "final_xgb = xgb.XGBClassifier(\n",
    "    nthread=10, max_depth=4, n_estimators=500, learning_rate=0.05)\n",
    "\n",
    "# fit the model with the selected features\n",
    "final_xgb.fit(X_train[features_to_keep], y_train)\n",
    "\n",
    "# make predictions\n",
    "y_pred_test = final_xgbl.predict_proba(X_test[features_to_keep])[:, 1]\n",
    "\n",
    "# calculate roc-auc\n",
    "auc_score_final = roc_auc_score(y_test, y_pred_test)\n",
    "print('Test selected features ROC AUC=%f' % (auc_score_final))\n",
    "print('Test all features ROC AUC=%f' % (auc_score_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the xgb model built with 56 features shows a similar performance than the one built with the total features (0.715 vs 0.713).\n",
    "\n",
    "We may not be able to get this right from the beginning though, as we did here. This method of feature selection does require that you try a few different tolerances / thresholds until you find the right number of features.\n",
    "\n",
    "Why don't you go ahead and try different values? Try with lower and bigger thresholds and get a feeling of how much this affects the number of selected features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 81)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "data = pd.read_csv('houseprice.csv', nrows=50000)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 38)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In practice, feature selection should be done after data pre-processing,\n",
    "# so ideally, all the categorical variables are encoded into numbers,\n",
    "# and then you can assess how deterministic they are of the target\n",
    "\n",
    "# here for simplicity I will use only numerical variables\n",
    "# select numerical columns:\n",
    "\n",
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "numerical_vars = list(data.select_dtypes(include=numerics).columns)\n",
    "data = data[numerical_vars]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1022, 36), (438, 36))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(labels=['Id','SalePrice'], axis=1),\n",
    "    data['SalePrice'],\n",
    "    test_size=0.3,\n",
    "    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test all features xgb R2 = 0.818551\n"
     ]
    }
   ],
   "source": [
    "# the first step of this procedure consists in building\n",
    "# a machine learning algorithm using all the available features\n",
    "# and then determine the importance of the features according\n",
    "# to the algorithm\n",
    "\n",
    "# set the seed for reproducibility\n",
    "seed_val = 1000000000\n",
    "np.random.seed(seed_val)\n",
    "\n",
    "# build initial model using all the features\n",
    "model_all_features = xgb.XGBRegressor(\n",
    "    nthread=10, max_depth=4, n_estimators=500, learning_rate=0.05)\n",
    "\n",
    "model_all_features.fit(X_train, y_train)\n",
    "\n",
    "# calculate the roc-auc in the test set\n",
    "y_pred_test = model_all_features.predict(X_test)\n",
    "r2_score_all = r2_score(y_test, y_pred_test)\n",
    "print('Test all features xgb R2 = %f' % (r2_score_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0xb71ed5cc18>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIUAAAGiCAYAAAB53XaEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xb71eace7f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the second step consist of deriving the importance of \n",
    "# each feature and ranking them from the least to the most\n",
    "# important\n",
    "\n",
    "# get feature name and importance\n",
    "features = pd.Series(model_all_features.feature_importances_)\n",
    "features.index = X_train.columns\n",
    "\n",
    "# sort the features by importance\n",
    "features.sort_values(ascending=True, inplace=True)\n",
    "\n",
    "# plot\n",
    "features.plot.bar(figsize=(20,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LowQualFinSF',\n",
       " 'KitchenAbvGr',\n",
       " 'BsmtFullBath',\n",
       " 'BsmtHalfBath',\n",
       " 'GarageCars',\n",
       " 'MiscVal',\n",
       " 'FullBath',\n",
       " 'HalfBath',\n",
       " 'Fireplaces',\n",
       " '3SsnPorch',\n",
       " 'EnclosedPorch',\n",
       " 'TotRmsAbvGrd',\n",
       " 'BedroomAbvGr',\n",
       " 'PoolArea',\n",
       " 'ScreenPorch',\n",
       " 'YrSold',\n",
       " 'BsmtFinSF2',\n",
       " '2ndFlrSF',\n",
       " 'YearRemodAdd',\n",
       " 'WoodDeckSF',\n",
       " 'OpenPorchSF',\n",
       " 'OverallQual',\n",
       " 'TotalBsmtSF',\n",
       " 'MasVnrArea',\n",
       " 'OverallCond',\n",
       " 'MSSubClass',\n",
       " 'MoSold',\n",
       " 'GarageYrBlt',\n",
       " 'YearBuilt',\n",
       " '1stFlrSF',\n",
       " 'BsmtFinSF1',\n",
       " 'GarageArea',\n",
       " 'GrLivArea',\n",
       " 'BsmtUnfSF',\n",
       " 'LotArea',\n",
       " 'LotFrontage']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view the list of ordered features\n",
    "features = list(features.index)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing recursive feature elimination\n",
      "\n",
      "testing feature:  LowQualFinSF  which is feature  1  out of  36\n",
      "New Test r2 = 0.8176628877400767\n",
      "All features Test r2 = 0.8185508615101988\n",
      "Drop in r2 = 0.0008879737701221746\n",
      "remove:  LowQualFinSF\n",
      "\n",
      "testing feature:  KitchenAbvGr  which is feature  2  out of  36\n",
      "New Test r2 = 0.8218740628640144\n",
      "All features Test r2 = 0.8176628877400767\n",
      "Drop in r2 = -0.00421117512393776\n",
      "remove:  KitchenAbvGr\n",
      "\n",
      "testing feature:  BsmtFullBath  which is feature  3  out of  36\n",
      "New Test r2 = 0.8216199474312881\n",
      "All features Test r2 = 0.8218740628640144\n",
      "Drop in r2 = 0.00025411543272635395\n",
      "remove:  BsmtFullBath\n",
      "\n",
      "testing feature:  BsmtHalfBath  which is feature  4  out of  36\n",
      "New Test r2 = 0.8241857928140686\n",
      "All features Test r2 = 0.8216199474312881\n",
      "Drop in r2 = -0.002565845382780485\n",
      "remove:  BsmtHalfBath\n",
      "\n",
      "testing feature:  GarageCars  which is feature  5  out of  36\n",
      "New Test r2 = 0.8234478263522329\n",
      "All features Test r2 = 0.8241857928140686\n",
      "Drop in r2 = 0.0007379664618356596\n",
      "remove:  GarageCars\n",
      "\n",
      "testing feature:  MiscVal  which is feature  6  out of  36\n",
      "New Test r2 = 0.8245560565485206\n",
      "All features Test r2 = 0.8234478263522329\n",
      "Drop in r2 = -0.0011082301962876961\n",
      "remove:  MiscVal\n",
      "\n",
      "testing feature:  FullBath  which is feature  7  out of  36\n",
      "New Test r2 = 0.8142130912663263\n",
      "All features Test r2 = 0.8245560565485206\n",
      "Drop in r2 =0.01034296528219425\n",
      "keep:  FullBath\n",
      "\n",
      "testing feature:  HalfBath  which is feature  8  out of  36\n",
      "New Test r2 = 0.8223914501432945\n",
      "All features Test r2 = 0.8245560565485206\n",
      "Drop in r2 =0.0021646064052260883\n",
      "keep:  HalfBath\n",
      "\n",
      "testing feature:  Fireplaces  which is feature  9  out of  36\n",
      "New Test r2 = 0.814062191310915\n",
      "All features Test r2 = 0.8245560565485206\n",
      "Drop in r2 =0.010493865237605648\n",
      "keep:  Fireplaces\n",
      "\n",
      "testing feature:  3SsnPorch  which is feature  10  out of  36\n",
      "New Test r2 = 0.8228884035226094\n",
      "All features Test r2 = 0.8245560565485206\n",
      "Drop in r2 =0.0016676530259112088\n",
      "keep:  3SsnPorch\n",
      "\n",
      "testing feature:  EnclosedPorch  which is feature  11  out of  36\n",
      "New Test r2 = 0.8216894685951396\n",
      "All features Test r2 = 0.8245560565485206\n",
      "Drop in r2 =0.002866587953381017\n",
      "keep:  EnclosedPorch\n",
      "\n",
      "testing feature:  TotRmsAbvGrd  which is feature  12  out of  36\n",
      "New Test r2 = 0.8200291572058384\n",
      "All features Test r2 = 0.8245560565485206\n",
      "Drop in r2 =0.004526899342682245\n",
      "keep:  TotRmsAbvGrd\n",
      "\n",
      "testing feature:  BedroomAbvGr  which is feature  13  out of  36\n",
      "New Test r2 = 0.8236742809864477\n",
      "All features Test r2 = 0.8245560565485206\n",
      "Drop in r2 = 0.0008817755620729173\n",
      "remove:  BedroomAbvGr\n",
      "\n",
      "testing feature:  PoolArea  which is feature  14  out of  36\n",
      "New Test r2 = 0.8123855405578879\n",
      "All features Test r2 = 0.8236742809864477\n",
      "Drop in r2 =0.011288740428559763\n",
      "keep:  PoolArea\n",
      "\n",
      "testing feature:  ScreenPorch  which is feature  15  out of  36\n",
      "New Test r2 = 0.8259976977255308\n",
      "All features Test r2 = 0.8236742809864477\n",
      "Drop in r2 = -0.002323416739083095\n",
      "remove:  ScreenPorch\n",
      "\n",
      "testing feature:  YrSold  which is feature  16  out of  36\n",
      "New Test r2 = 0.8238306513646853\n",
      "All features Test r2 = 0.8259976977255308\n",
      "Drop in r2 =0.002167046360845526\n",
      "keep:  YrSold\n",
      "\n",
      "testing feature:  BsmtFinSF2  which is feature  17  out of  36\n",
      "New Test r2 = 0.8170781108452227\n",
      "All features Test r2 = 0.8259976977255308\n",
      "Drop in r2 =0.008919586880308028\n",
      "keep:  BsmtFinSF2\n",
      "\n",
      "testing feature:  2ndFlrSF  which is feature  18  out of  36\n",
      "New Test r2 = 0.816373763701784\n",
      "All features Test r2 = 0.8259976977255308\n",
      "Drop in r2 =0.009623934023746727\n",
      "keep:  2ndFlrSF\n",
      "\n",
      "testing feature:  YearRemodAdd  which is feature  19  out of  36\n",
      "New Test r2 = 0.8258583975235964\n",
      "All features Test r2 = 0.8259976977255308\n",
      "Drop in r2 = 0.0001393002019344225\n",
      "remove:  YearRemodAdd\n",
      "\n",
      "testing feature:  WoodDeckSF  which is feature  20  out of  36\n",
      "New Test r2 = 0.8211710487205289\n",
      "All features Test r2 = 0.8258583975235964\n",
      "Drop in r2 =0.004687348803067426\n",
      "keep:  WoodDeckSF\n",
      "\n",
      "testing feature:  OpenPorchSF  which is feature  21  out of  36\n",
      "New Test r2 = 0.8224357188998381\n",
      "All features Test r2 = 0.8258583975235964\n",
      "Drop in r2 =0.003422678623758224\n",
      "keep:  OpenPorchSF\n",
      "\n",
      "testing feature:  OverallQual  which is feature  22  out of  36\n",
      "New Test r2 = 0.7923627378533993\n",
      "All features Test r2 = 0.8258583975235964\n",
      "Drop in r2 =0.033495659670197075\n",
      "keep:  OverallQual\n",
      "\n",
      "testing feature:  TotalBsmtSF  which is feature  23  out of  36\n",
      "New Test r2 = 0.8159251371651921\n",
      "All features Test r2 = 0.8258583975235964\n",
      "Drop in r2 =0.00993326035840425\n",
      "keep:  TotalBsmtSF\n",
      "\n",
      "testing feature:  MasVnrArea  which is feature  24  out of  36\n",
      "New Test r2 = 0.8531558041299072\n",
      "All features Test r2 = 0.8258583975235964\n",
      "Drop in r2 = -0.02729740660631086\n",
      "remove:  MasVnrArea\n",
      "\n",
      "testing feature:  OverallCond  which is feature  25  out of  36\n",
      "New Test r2 = 0.8446694034811906\n",
      "All features Test r2 = 0.8531558041299072\n",
      "Drop in r2 =0.008486400648716641\n",
      "keep:  OverallCond\n",
      "\n",
      "testing feature:  MSSubClass  which is feature  26  out of  36\n",
      "New Test r2 = 0.851514246867042\n",
      "All features Test r2 = 0.8531558041299072\n",
      "Drop in r2 =0.0016415572628651898\n",
      "keep:  MSSubClass\n",
      "\n",
      "testing feature:  MoSold  which is feature  27  out of  36\n",
      "New Test r2 = 0.863875155480818\n",
      "All features Test r2 = 0.8531558041299072\n",
      "Drop in r2 = -0.010719351350910733\n",
      "remove:  MoSold\n",
      "\n",
      "testing feature:  GarageYrBlt  which is feature  28  out of  36\n",
      "New Test r2 = 0.8607770586428112\n",
      "All features Test r2 = 0.863875155480818\n",
      "Drop in r2 =0.0030980968380067697\n",
      "keep:  GarageYrBlt\n",
      "\n",
      "testing feature:  YearBuilt  which is feature  29  out of  36\n",
      "New Test r2 = 0.8597803158007506\n",
      "All features Test r2 = 0.863875155480818\n",
      "Drop in r2 =0.004094839680067297\n",
      "keep:  YearBuilt\n",
      "\n",
      "testing feature:  1stFlrSF  which is feature  30  out of  36\n",
      "New Test r2 = 0.8571840255319483\n",
      "All features Test r2 = 0.863875155480818\n",
      "Drop in r2 =0.006691129948869667\n",
      "keep:  1stFlrSF\n",
      "\n",
      "testing feature:  BsmtFinSF1  which is feature  31  out of  36\n",
      "New Test r2 = 0.8535897492997575\n",
      "All features Test r2 = 0.863875155480818\n",
      "Drop in r2 =0.010285406181060441\n",
      "keep:  BsmtFinSF1\n",
      "\n",
      "testing feature:  GarageArea  which is feature  32  out of  36\n",
      "New Test r2 = 0.8619347524274193\n",
      "All features Test r2 = 0.863875155480818\n",
      "Drop in r2 =0.0019404030533985983\n",
      "keep:  GarageArea\n",
      "\n",
      "testing feature:  GrLivArea  which is feature  33  out of  36\n",
      "New Test r2 = 0.8576575601588515\n",
      "All features Test r2 = 0.863875155480818\n",
      "Drop in r2 =0.006217595321966418\n",
      "keep:  GrLivArea\n",
      "\n",
      "testing feature:  BsmtUnfSF  which is feature  34  out of  36\n",
      "New Test r2 = 0.8654469148741569\n",
      "All features Test r2 = 0.863875155480818\n",
      "Drop in r2 = -0.0015717593933389784\n",
      "remove:  BsmtUnfSF\n",
      "\n",
      "testing feature:  LotArea  which is feature  35  out of  36\n",
      "New Test r2 = 0.8532509341202452\n",
      "All features Test r2 = 0.8654469148741569\n",
      "Drop in r2 =0.012195980753911706\n",
      "keep:  LotArea\n",
      "\n",
      "testing feature:  LotFrontage  which is feature  36  out of  36\n",
      "New Test r2 = 0.887427764266262\n",
      "All features Test r2 = 0.8654469148741569\n",
      "Drop in r2 = -0.021980849392105095\n",
      "remove:  LotFrontage\n",
      "DONE!!\n",
      "total features to remove:  13\n",
      "total features to keep:  23\n"
     ]
    }
   ],
   "source": [
    "# the final step consists in removing one at a time\n",
    "# all the features, from the least to the most\n",
    "# important, and build an xgboost at each round.\n",
    "\n",
    "# once we build the model, we calculate the new r2\n",
    "# if the new r2 is smaller than the original one\n",
    "# (with all the features), then that feature that was removed\n",
    "# was important, and we should keep it.\n",
    "# otherwise, we should remove the feature\n",
    "\n",
    "# recursive feature elimination:\n",
    "\n",
    "# first we arbitrarily set the drop in r2\n",
    "# if the drop is below this threshold,\n",
    "# the feature will be removed\n",
    "tol = 0.001\n",
    "\n",
    "print('doing recursive feature elimination')\n",
    "\n",
    "# we initialise a list where we will collect the\n",
    "# features we should remove\n",
    "features_to_remove = []\n",
    "\n",
    "# set a counter to know how far ahead the loop is going\n",
    "count = 1\n",
    "\n",
    "# now we loop over all the features, in order of importance:\n",
    "# remember that features is the list of ordered features\n",
    "# by importance\n",
    "for feature in features:\n",
    "    print()\n",
    "    print('testing feature: ', feature, ' which is feature ', count,\n",
    "          ' out of ', len(features))\n",
    "    count = count + 1\n",
    "\n",
    "    # initialise model\n",
    "    model_int = xgb.XGBRegressor(\n",
    "        nthread=10, max_depth=4, n_estimators=500, learning_rate=0.05)\n",
    "\n",
    "    # fit model with all variables minus the removed features\n",
    "    # and the feature to be evaluated\n",
    "    model_int.fit(\n",
    "        X_train.drop(features_to_remove + [feature], axis=1), y_train)\n",
    "\n",
    "    # make a prediction over the test set\n",
    "    y_pred_test = model_int.predict(\n",
    "        X_test.drop(features_to_remove + [feature], axis=1))\n",
    "\n",
    "    # calculate the new r2\n",
    "    r2_score_int = r2_score(y_test, y_pred_test)\n",
    "    print('New Test r2 = {}'.format((r2_score_int)))\n",
    "\n",
    "    # print the original r2 with all the features\n",
    "    print('All features Test r2 = {}'.format((r2_score_all)))\n",
    "\n",
    "    # determine the drop in the r2\n",
    "    diff_r2 = r2_score_all - r2_score_int\n",
    "\n",
    "    # compare the drop in r2 with the tolerance\n",
    "    # we set previously\n",
    "    if diff_r2 >= tol:\n",
    "        print('Drop in r2 ={}'.format(diff_r2))\n",
    "        print('keep: ', feature)\n",
    "        print\n",
    "    else:\n",
    "        print('Drop in r2 = {}'.format(diff_r2))\n",
    "        print('remove: ', feature)\n",
    "        print\n",
    "        # if the drop in the r2 is small and we remove the\n",
    "        # feature, we need to set the new r2 to the one based on\n",
    "        # the remaining features\n",
    "        r2_score_all = r2_score_int\n",
    "        \n",
    "        # and append the feature to remove to the collecting\n",
    "        # list\n",
    "        features_to_remove.append(feature)\n",
    "\n",
    "# now the loop is finished, we evaluated all the features\n",
    "print('DONE!!')\n",
    "print('total features to remove: ', len(features_to_remove))\n",
    "\n",
    "# determine the features to keep (those we won't remove)\n",
    "features_to_keep = [x for x in features if x not in features_to_remove]\n",
    "print('total features to keep: ', len(features_to_keep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test selected features r2 = 0.863848\n",
      "Test all features r2 = 0.887428\n"
     ]
    }
   ],
   "source": [
    "# capture the 23 selected features\n",
    "seed_val = 1000000000\n",
    "np.random.seed(seed_val)\n",
    "\n",
    "# build initial model\n",
    "final_xgb = xgb.XGBRegressor(\n",
    "    nthread=10, max_depth=4, n_estimators=500, learning_rate=0.05)\n",
    "\n",
    "# fit the model with the selected features\n",
    "final_xgb.fit(X_train[features_to_keep], y_train)\n",
    "\n",
    "# make predictions\n",
    "y_pred_test = final_xgb.predict(X_test[features_to_keep])\n",
    "\n",
    "# calculate roc-auc\n",
    "r2_score_final = r2_score(y_test, y_pred_test)\n",
    "print('Test selected features r2 = %f' % (r2_score_final))\n",
    "print('Test all features r2 = %f' % (r2_score_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model built with 23 features seems to be more predictive than the one built with the total number of features (r2 = 0.88 vs 0.81). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
